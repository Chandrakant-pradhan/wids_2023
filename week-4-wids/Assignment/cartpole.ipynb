{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving CartPole with DQNs\n",
    "In this assignment you will make an RL agent capable of achieving 150+ average reward in the CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all necessary imports here\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the CartPoleAgent class:\n",
    "- The constructor (\\_\\___init__\\_\\_) should initialize __gamma__ and __epsilon__ as class variables. It initializes online network, saves it and loads it again in target network (We do this so that both our target and online network are same during initialization)\n",
    "- The __choose_action()__ function should take the __Q(s, a)__ values vector for a state s as input, for example if __Q_s__ is the given input, __Q_s[0]__ represents __Q(s, 0)__, __Q_s[1]__ represents __Q(s, 1)__ and so on, and the function should output the chosen action (an integer) according to the current exploration strategy (For example choose random action with probability ε and choose action with highest Q(s, a) value with probability 1-ε)\n",
    "- The __train()__ function runs for a specific number of loops, in each loop:\n",
    "    - It generates training data using __generate_training_data()__ function and passes it to train_instance function of the online network (which trains the online network)\n",
    "    - It then saves the online network and loads that same saved function as target network\n",
    "    - Calls the __evaluate_performace()__ function\n",
    "    - Updates the value of epsilon as required\n",
    "- The __generate_training_data()__ function:\n",
    "    - Simulates lots of episodes/games/trajectories, it uses the online network for chossing actions, and the target netowrk for determining targets, it then stores all such states in an list/array/tensor and corresponding labels (i.e. targets) in another list/array/tensor.\n",
    "    - It then makes a __CustomDataset__ variable with these state and labels and returns it\n",
    "    - The CartPole environment terminates after 500 steps truncates itself after 500 steps in a single episode, you have to check this yourself and terminate the episode if it's length becomes >= 500\n",
    "    - The number of data and targets in the dataset returned should be large enough (around 5000-10000), so that when we choose any random datapoints, they satisy the iid condition\n",
    "- The __evaluate_performance()__ function calculates the average achieved reward with the current online network by simulating atleast 5 episodes (without any exploration as we are just calculating average reward), it then prints the average reward\n",
    "\n",
    "Generally you should see a rising trend in your average obtained reward\n",
    "\n",
    "Now some recommendations:\n",
    "- You need a good exploratory strategy, exponentially decaying exploration is prefered, you can start with ε=0.5 and then divide it by a constant after each training loop, so that it finally reaches a value of ε = 0.01\n",
    "- Whenever you use forward function of the DQN class in __generate_training_data()__ or __evaluate()__, make sure to detach the tensor so that it does not calculate gradients. You can detach any tensor \"__a__\" like:\n",
    "```\n",
    "    a = a.detach()\n",
    "```\n",
    "- 0.99 is a good value for Gamma\n",
    "\n",
    "Some more things you can do (Optional):\n",
    "- You can load an already saved PyTorch model with name \"model.pth\" into any variable network as follows:\n",
    "```\n",
    "    network = torch.load(\"model.pth)\n",
    "```\n",
    "- In the __evaluate()__ function, you can use __imageio__ library to make gifs of your agent playing the game (Google How!), but you have to initialize your environment as:\n",
    "```\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "```\n",
    "- In the __evaluate()__ function, you can calculate the Mean-Square Error of the model and store these values for each iterations and finally plot it to get an idea of how is your training going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''running perfectly fine on google colab'''\n",
    "class CartPoleAgent:\n",
    "    def __init__(self, epsilon, gamma=0.99) -> None:\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.online_network = DQN(input_size=4, output_size=2)  # Corrected input_size to 4\n",
    "        self.target_network = DQN(input_size=4, output_size=2)  # Corrected input_size to 4\n",
    "        self.target_network.load_state_dict(self.online_network.state_dict())\n",
    "        self.env = gym.make('CartPole-v1')  # Initialize environment\n",
    "\n",
    "    def choose_action(self, Q_s) -> int:\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(len(Q_s))\n",
    "        else:\n",
    "            return np.argmax(Q_s)\n",
    "\n",
    "    def generate_training_data(self) -> CustomDataset:\n",
    "        data = []\n",
    "        labels = []\n",
    "\n",
    "        for _ in range(100):\n",
    "            state = self.env.reset()\n",
    "            state = np.append(state, self.choose_action(self.online_network(torch.tensor(state, dtype=torch.float32)).detach().numpy()))\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.choose_action(self.online_network(torch.tensor(state[:4], dtype=torch.float32)).detach().numpy())\n",
    "                next_state, env_reward, done, _ = self.env.step(action)\n",
    "                next_state = np.append(next_state, self.choose_action(self.online_network(torch.tensor(next_state, dtype=torch.float32)).detach().numpy()))\n",
    "\n",
    "                reward = env_reward - (abs(state[0]) + abs(state[2])) / 2.5\n",
    "                total_reward += reward\n",
    "                target = np.zeros(2)  # One value for each action\n",
    "                target[action] = reward + self.gamma * np.max(self.target_network(torch.tensor(next_state[:4], dtype=torch.float32)).detach().numpy())\n",
    "\n",
    "                data.append(state[:4])\n",
    "                labels.append(target)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done or len(data) >= 5000:\n",
    "                    break\n",
    "\n",
    "        return CustomDataset(np.array(data), np.array(labels))\n",
    "\n",
    "    def train_agent(self, num_loops=100):\n",
    "        for _ in range(num_loops):\n",
    "            train_dataset = self.generate_training_data()\n",
    "            self.online_network.train_instance(train_dataset)\n",
    "\n",
    "            self.online_network.save_model('model.pth')\n",
    "            self.target_network.load_state_dict(self.online_network.state_dict())\n",
    "\n",
    "            self.evaluate_performance(_)\n",
    "            self.epsilon *= 0.99\n",
    "\n",
    "    def evaluate_performance(self, iter) -> None:\n",
    "        total_reward = 0\n",
    "        for _ in range(5):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                # Choose action using the online network\n",
    "                action = np.argmax(self.online_network(torch.tensor(state, dtype=torch.float32)).detach().numpy())\n",
    "\n",
    "                state, env_reward, done, _ = self.env.step(action)\n",
    "                total_reward += env_reward\n",
    "\n",
    "        print(f'Iteration {iter + 1}, Average Reward: {total_reward / 5}')\n",
    "\n",
    "    def play_and_save_gif(self, num_episodes=5):\n",
    "        frames = []  # List to store frames for GIF\n",
    "\n",
    "        for _ in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                # Choose action using the online network\n",
    "                action = np.argmax(self.online_network(torch.tensor(state, dtype=torch.float32)).detach().numpy())\n",
    "\n",
    "                state, _, done, _ = self.env.step(action)\n",
    "\n",
    "                # Append the current frame to the frames list\n",
    "                frames.append(self.env.render(mode='rgb_array'))\n",
    "\n",
    "        # Save the frames as a GIF\n",
    "        gif_path = 'cartpole_play.gif'\n",
    "        imageio.mimsave(gif_path, frames, fps=30)\n",
    "\n",
    "        # Display the GIF in Colab\n",
    "        with open(gif_path, 'rb') as f:\n",
    "            display(Image(data=f.read(), format='png'))\n",
    "\n",
    "        print(f'GIF saved at: {gif_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should run the below cell to start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 4 at dim 1 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This cell should not be changed\u001b[39;00m\n\u001b[0;32m      2\u001b[0m Agent \u001b[38;5;241m=\u001b[39m CartPoleAgent(epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m Agent\u001b[38;5;241m.\u001b[39mplay_and_save_gif(num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 48\u001b[0m, in \u001b[0;36mCartPoleAgent.train_agent\u001b[1;34m(self, num_loops)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_agent\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_loops\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_loops):\n\u001b[1;32m---> 48\u001b[0m         train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monline_network\u001b[38;5;241m.\u001b[39mtrain_instance(train_dataset)\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monline_network\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m, in \u001b[0;36mCartPoleAgent.generate_training_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     21\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m---> 22\u001b[0m     state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoose_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monline_network(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[0;32m     23\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 4 at dim 1 (got 0)"
     ]
    }
   ],
   "source": [
    "'''running perfectly fine on google colab'''\n",
    "# This cell should not be changed\n",
    "Agent = CartPoleAgent(epsilon=0.5)\n",
    "Agent.train_agent()\n",
    "Agent.play_and_save_gif(num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
