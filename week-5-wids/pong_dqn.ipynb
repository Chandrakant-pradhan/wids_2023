{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_nudb3jvgsbU"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "WORKING IN GOOGLE COLAB\n",
    "\n",
    "'''\n",
    "\n",
    "import gym\n",
    "import cv2\n",
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "conv = namedtuple('Conv', 'filter kernel stride')\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def add(self, s, a, r, s2, t):\n",
    "        s = np.stack((s[0], s[1], s[2], s[3]), axis=2)\n",
    "        s2 = np.stack((s2[0], s2[1], s2[2], s2[3]), axis=2)\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.appendleft((s, a, r, s2, t))\n",
    "        else:\n",
    "            self.buffer.pop()\n",
    "            self.buffer.appendleft((s, a, r, s2, t))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, buff, batch_size=32, min_buff=10000, gamma=0.99, learning_rate=2.5e-4):\n",
    "        self.buffer = buff\n",
    "        self.min_buffer = min_buff\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.model = create_network(learning_rate)\n",
    "        self.target_model = create_network(learning_rate)\n",
    "        self.copy_network()\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.buffer.buffer) < self.min_buffer:\n",
    "            return\n",
    "        states, actions, rewards, next_states, terminal = map(np.array, zip(*self.buffer.sample(self.batch_size)))\n",
    "        next_state_action_values = np.max(self.target_model.predict(next_states), axis=1)\n",
    "        targets = self.model.predict(states)\n",
    "        targets[range(self.batch_size), actions] = rewards + self.gamma * next_state_action_values * np.invert(terminal)\n",
    "        self.model.train_on_batch(states, targets)\n",
    "\n",
    "    def copy_network(self):\n",
    "        frm = self.model\n",
    "        to = self.target_model\n",
    "        for l_tg, l_sr in zip(to.layers, frm.layers):\n",
    "            wk = l_sr.get_weights()\n",
    "            l_tg.set_weights(wk)\n",
    "\n",
    "    def predict(self, x):\n",
    "        s = np.stack((x[0], x[1], x[2], x[3]), axis=2)\n",
    "        return self.model.predict(np.array([s]))\n",
    "\n",
    "\n",
    "def create_network(learning_rate, conv_info=[conv(32, 8, 4), conv(64, 4, 2), conv(64, 3, 1)], dense_info=[512],\n",
    "                   input_size=(80, 80, 4)):\n",
    "    model = Sequential()\n",
    "    for i, cl in enumerate(conv_info):\n",
    "        if i == 0:\n",
    "            model.add(Conv2D(cl.filter, cl.kernel, padding=\"same\", strides=cl.stride, activation=\"relu\",\n",
    "                             input_shape=input_size))\n",
    "        else:\n",
    "            model.add(Conv2D(cl.filter, cl.kernel, padding=\"same\", strides=cl.stride, activation=\"relu\"))\n",
    "    model.add(Flatten())\n",
    "    for dl in dense_info:\n",
    "        model.add(Dense(dl, activation=\"relu\"))\n",
    "    model.add(Dense(6))\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=adam)\n",
    "    return model\n",
    "\n",
    "\n",
    "class Pong:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('Pong-v0')\n",
    "        self.epsilon = 1\n",
    "        self.buffer = Buffer(50000)\n",
    "        self.dqn = DQN(self.buffer)\n",
    "        self.copy_period = 40000\n",
    "        self.itr = 0\n",
    "        self.eps_step = 0.0000009\n",
    "\n",
    "    def sample_action(self, s):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.dqn.predict(s)[0])\n",
    "\n",
    "    def play_one_episode(self):\n",
    "        observation = self.env.reset()\n",
    "        done = False\n",
    "        state = []\n",
    "        update_state(state, observation)\n",
    "        prv_state = []\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if len(state) < 4:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                action = self.sample_action(state)\n",
    "\n",
    "            prv_state.append(state[-1])\n",
    "            if len(prv_state) > 4:\n",
    "                prv_state.pop(0)\n",
    "\n",
    "        # Update the state using the step result\n",
    "            step_result = self.env.step(action)\n",
    "            observation, reward, done, _ = step_result[:4]\n",
    "\n",
    "            update_state(state, observation)\n",
    "            if len(state) == 4 and len(prv_state) == 4:\n",
    "                self.buffer.add(prv_state, action, reward, state, done)\n",
    "            total_reward += reward\n",
    "\n",
    "            self.itr += 1\n",
    "            if self.itr % 4 == 0:\n",
    "                self.dqn.train()\n",
    "            self.epsilon = max(0.1, self.epsilon - self.eps_step)\n",
    "            if self.itr % self.copy_period == 0:\n",
    "                self.dqn.copy_network()\n",
    "\n",
    "            return total_reward\n",
    "\n",
    "\n",
    "\n",
    "def downsample(observation):\n",
    "    s = cv2.cvtColor(observation[30:, :, :], cv2.COLOR_BGR2GRAY)\n",
    "    s = cv2.resize(s, (80, 80), interpolation=cv2.INTER_AREA)\n",
    "    s = s / 255.0\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_state(state, observation):\n",
    "    ds_observation = downsample(observation[0])\n",
    "    state.append(ds_observation)\n",
    "    if len(state) > 4:\n",
    "        state.pop(0)\n",
    "\n",
    "\n",
    "p = Pong()\n",
    "for i in range(100000):\n",
    "    total_reward = p.play_one_episode()\n",
    "    print(\"episode total reward:\", total_reward)\n",
    "    if i % 100 == 0:\n",
    "        print(\"Saving the model\")\n",
    "        p.dqn.model.save(\"model-{}.h5\".format(i))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "pong_dqn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
