{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_nudb3jvgsbU"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 145\u001b[0m\n\u001b[0;32m    143\u001b[0m p \u001b[38;5;241m=\u001b[39m Pong()\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100000\u001b[39m):\n\u001b[1;32m--> 145\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_one_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode total reward:\u001b[39m\u001b[38;5;124m\"\u001b[39m, total_reward)\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[4], line 100\u001b[0m, in \u001b[0;36mPong.play_one_episode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     99\u001b[0m state \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 100\u001b[0m \u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m prv_state \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    102\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[4], line 137\u001b[0m, in \u001b[0;36mupdate_state\u001b[1;34m(state, observation)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_state\u001b[39m(state, observation):\n\u001b[1;32m--> 137\u001b[0m     ds_observation \u001b[38;5;241m=\u001b[39m \u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m     state\u001b[38;5;241m.\u001b[39mappend(ds_observation)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "Cell \u001b[1;32mIn[4], line 130\u001b[0m, in \u001b[0;36mdownsample\u001b[1;34m(observation)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownsample\u001b[39m(observation):\n\u001b[1;32m--> 130\u001b[0m     s \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(\u001b[43mobservation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m    131\u001b[0m     s \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(s, (\u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m80\u001b[39m), interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_AREA)\n\u001b[0;32m    132\u001b[0m     s \u001b[38;5;241m=\u001b[39m s \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import cv2\n",
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "conv = namedtuple('Conv', 'filter kernel stride')\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def add(self, s, a, r, s2, t):\n",
    "        s = np.stack((s[0], s[1], s[2], s[3]), axis=2)\n",
    "        s2 = np.stack((s2[0], s2[1], s2[2], s2[3]), axis=2)\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.appendleft((s, a, r, s2, t))\n",
    "        else:\n",
    "            self.buffer.pop()\n",
    "            self.buffer.appendleft((s, a, r, s2, t))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, buff, batch_size=32, min_buff=10000, gamma=0.99, learning_rate=2.5e-4):\n",
    "        self.buffer = buff\n",
    "        self.min_buffer = min_buff\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.model = create_network(learning_rate)\n",
    "        self.target_model = create_network(learning_rate)\n",
    "        self.copy_network()\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.buffer.buffer) < self.min_buffer:\n",
    "            return\n",
    "        states, actions, rewards, next_states, terminal = map(np.array, zip(*self.buffer.sample(self.batch_size)))\n",
    "        next_state_action_values = np.max(self.target_model.predict(next_states), axis=1)\n",
    "        targets = self.model.predict(states)\n",
    "        targets[range(self.batch_size), actions] = rewards + self.gamma * next_state_action_values * np.invert(terminal)\n",
    "        self.model.train_on_batch(states, targets)\n",
    "\n",
    "    def copy_network(self):\n",
    "        frm = self.model\n",
    "        to = self.target_model\n",
    "        for l_tg, l_sr in zip(to.layers, frm.layers):\n",
    "            wk = l_sr.get_weights()\n",
    "            l_tg.set_weights(wk)\n",
    "\n",
    "    def predict(self, x):\n",
    "        s = np.stack((x[0], x[1], x[2], x[3]), axis=2)\n",
    "        return self.model.predict(np.array([s]))\n",
    "\n",
    "\n",
    "def create_network(learning_rate, conv_info=[conv(32, 8, 4), conv(64, 4, 2), conv(64, 3, 1)], dense_info=[512],\n",
    "                   input_size=(80, 80, 4)):\n",
    "    model = Sequential()\n",
    "    for i, cl in enumerate(conv_info):\n",
    "        if i == 0:\n",
    "            model.add(Conv2D(cl.filter, cl.kernel, padding=\"same\", strides=cl.stride, activation=\"relu\",\n",
    "                             input_shape=input_size))\n",
    "        else:\n",
    "            model.add(Conv2D(cl.filter, cl.kernel, padding=\"same\", strides=cl.stride, activation=\"relu\"))\n",
    "    model.add(Flatten())\n",
    "    for dl in dense_info:\n",
    "        model.add(Dense(dl, activation=\"relu\"))\n",
    "    model.add(Dense(6))\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=adam)\n",
    "    return model\n",
    "\n",
    "\n",
    "class Pong:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('Pong-v0')\n",
    "        self.epsilon = 1\n",
    "        self.buffer = Buffer(50000)\n",
    "        self.dqn = DQN(self.buffer)\n",
    "        self.copy_period = 40000\n",
    "        self.itr = 0\n",
    "        self.eps_step = 0.0000009\n",
    "\n",
    "    def sample_action(self, s):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.dqn.predict(s)[0])\n",
    "\n",
    "    def play_one_episode(self):\n",
    "        observation = self.env.reset()\n",
    "        done = False\n",
    "        state = []\n",
    "        update_state(state, observation)\n",
    "        prv_state = []\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "\n",
    "            if len(state) < 4:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                action = self.sample_action(state)\n",
    "\n",
    "            prv_state.append(state[-1])\n",
    "            if len(prv_state) > 4:\n",
    "                prv_state.pop(0)\n",
    "            observation, reward, done, _ = self.env.step(action)\n",
    "\n",
    "            update_state(state, observation)\n",
    "            if len(state) == 4 and len(prv_state) == 4:\n",
    "                self.buffer.add(prv_state, action, reward, state, done)\n",
    "            total_reward += reward\n",
    "\n",
    "            self.itr += 1\n",
    "            if self.itr % 4 == 0:\n",
    "                self.dqn.train()\n",
    "            self.epsilon = max(0.1, self.epsilon - self.eps_step)\n",
    "            if self.itr % self.copy_period == 0:\n",
    "                self.dqn.copy_network()\n",
    "        return total_reward\n",
    "\n",
    "\n",
    "def downsample(observation):\n",
    "    s = cv2.cvtColor(observation[30:, :, :], cv2.COLOR_BGR2GRAY)\n",
    "    s = cv2.resize(s, (80, 80), interpolation=cv2.INTER_AREA)\n",
    "    s = s / 255.0\n",
    "    return s\n",
    "\n",
    "\n",
    "def update_state(state, observation):\n",
    "    ds_observation = downsample(observation)\n",
    "    state.append(ds_observation)\n",
    "    if len(state) > 4:\n",
    "        state.pop(0)\n",
    "\n",
    "\n",
    "p = Pong()\n",
    "for i in range(100000):\n",
    "    total_reward = p.play_one_episode()\n",
    "    print(\"episode total reward:\", total_reward)\n",
    "    if i % 100 == 0:\n",
    "        print(\"Saving the model\")\n",
    "        p.dqn.model.save(\"model-{}.h5\".format(i))\n",
    "# correct this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "pong_dqn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
